{"meta":{"page":{"currentPage":1,"from":1,"lastPage":41,"perPage":4,"to":4,"total":164}},"jsonapi":{"version":"1.0"},"links":{"first":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications?filter%5Bpublished%5D=true&include=media&page%5Bnumber%5D=1&page%5Bsize%5D=4&sort=-publicationDate%2C-createdAt","last":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications?filter%5Bpublished%5D=true&include=media&page%5Bnumber%5D=41&page%5Bsize%5D=4&sort=-publicationDate%2C-createdAt","next":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications?filter%5Bpublished%5D=true&include=media&page%5Bnumber%5D=2&page%5Bsize%5D=4&sort=-publicationDate%2C-createdAt"},"data":[{"type":"research-publications","id":"228","attributes":{"published":true,"createdAt":"2023-08-03T17:45:52.000000Z","updatedAt":"2023-08-03T18:44:58.000000Z","publicationDateFormatted":"August 1, 2023","publicationDate":"2023-08-01T00:00:00-07:00","slug":"confidence-building-measures-for-artificial-intelligence","title":"Confidence-Building Measures for Artificial Intelligence: Workshop proceedings","description":"","descriptionRichText":null,"heroBlendMode":"default","heroTitle":null,"template":"level-0","colorTheme":null,"releaseSummary":null,"releaseWhyItMatters":null,"releaseSafety":null,"chapters":false,"publicationAbstract":"<p>Foundation models could eventually introduce several pathways for undermining state security: accidents, inadvertent escalation, unintentional conflict, the proliferation of weapons, and the interference with human diplomacy are just a few on a long list. The Confidence-Building Measures for Artificial Intelligence workshop hosted by the Geopolitics Team at OpenAI and the Berkeley Risk and Security Lab at the University of California brought together a multistakeholder group to think through the tools and strategies to mitigate the potential risks introduced by foundation models to international security. Originating in the Cold War, confidence-building measures (CBMs) are actions that reduce hostility, prevent conflict escalation, and improve trust between parties. The flexibility of CBMs make them a key instrument for navigating the rapid changes in the foundation model landscape. Participants identified the following CBMs that directly apply to foundation models and which are further explained in this conference proceedings: 1. crisis hotlines 2. incident sharing 3. model, transparency, and system cards 4. content provenance and watermarks 5. collaborative red teaming and table-top exercises and 6. dataset and evaluation sharing. Because most foundation model developers are non-government entities, many CBMs will need to involve a wider stakeholder community. These measures can be implemented either by AI labs or by relevant government actors.<br class=\"softbreak\"><\/p>","publicationAbstractRichText":{"type":"div","props":[],"children":[{"type":"p","props":[],"children":["Foundation models could eventually introduce several pathways for undermining state security: accidents, inadvertent escalation, unintentional conflict, the proliferation of weapons, and the interference with human diplomacy are just a few on a long list. The Confidence-Building Measures for Artificial Intelligence workshop hosted by the Geopolitics Team at OpenAI and the Berkeley Risk and Security Lab at the University of California brought together a multistakeholder group to think through the tools and strategies to mitigate the potential risks introduced by foundation models to international security. Originating in the Cold War, confidence-building measures (CBMs) are actions that reduce hostility, prevent conflict escalation, and improve trust between parties. The flexibility of CBMs make them a key instrument for navigating the rapid changes in the foundation model landscape. Participants identified the following CBMs that directly apply to foundation models and which are further explained in this conference proceedings: 1. crisis hotlines 2. incident sharing 3. model, transparency, and system cards 4. content provenance and watermarks 5. collaborative red teaming and table-top exercises and 6. dataset and evaluation sharing. Because most foundation model developers are non-government entities, many CBMs will need to involve a wider stakeholder community. These measures can be implemented either by AI labs or by relevant government actors.",{"type":"br","props":{"class":"softbreak"},"children":[]}]}]},"seo":{"ogImageSrc":"https:\/\/images.openai.com\/blob\/169a9863-5725-45cf-b096-6d2e5b6cebe9\/confidence-building-measures-for-artificial-intelligence-workshop-proceedings.png?trim=0%2C0%2C0%2C0","ogImageAlt":"Confidence Building Measures For Artificial Intelligence: Workshop Proceedings","twitterImageSrc":null,"twitterImageAlt":null,"noindex":false,"description":"","ogTitle":"Confidence-Building Measures for Artificial Intelligence: Workshop proceedings","ogDescription":"","twitterTitle":null,"twitterDescription":null,"twitterCardType":"summary_large_image"},"acknowledgments":{"title1":"Report authors, in order of contribution","text1":"<p>Sarah Shoker (OpenAI)*<br class=\"softbreak\">Andrew Reddie (University of California, Berkeley)**<br class=\"softbreak\"><\/p>","title2":"Report authors, in alphabetical order","text2":"<p>Sarah Barrington (University of California, Berkeley)<br class=\"softbreak\">Ruby Booth (Berkeley Risk and Security Lab)<br class=\"softbreak\">Miles Brundage (OpenAI)<br class=\"softbreak\">Husanjot Chahal (OpenAI)<br class=\"softbreak\">Michael Depp (Center for a New American Security)<br class=\"softbreak\">Bill Drexel (Center for a New American Security)<br class=\"softbreak\">Ritwik Gupta (University of California, Berkeley)<br class=\"softbreak\">Marina Favaro (Anthropic)<br class=\"softbreak\">Jake Hecla (University of California, Berkeley)<br class=\"softbreak\">Alan Hickey (OpenAI)<br class=\"softbreak\">Margarita Konaev (Center for Security and Emerging Technology)<br class=\"softbreak\">Kirthi Kumar (University of California, Berkeley)<br class=\"softbreak\">Nathan Lambert (Hugging Face)<br class=\"softbreak\">Andrew Lohn (Center for Security and Emerging Technology)<br class=\"softbreak\">Cullen O'Keefe (OpenAI)<br class=\"softbreak\">Nazneen Rajani (Hugging Face)<br class=\"softbreak\">Michael Sellitto (Anthropic)<br class=\"softbreak\">Robert Trager (Centre for the Governance of AI)<br class=\"softbreak\">Leah Walker (University of California, Berkeley)<br class=\"softbreak\">Alexa Wehsener (Institute for Security and Technology)<br class=\"softbreak\">Jessica Young (Microsoft)<\/p><p><br><\/p><p>All authors provided substantive contributions to the paper through sharing their ideas as participants in the workshop, writing the paper, and\/or editorial feedback and direction. The first two authors are listed in order of contribution, and the remaining authors are listed alphabetically. Some workshop participants have chosen to remain anonymous. The claims in this paper do not represent the views of any author\u2019s organization. For questions about this paper, contact Sarah Shoker at <a href=\"mailto:sshoker@openai.com\" rel=\"noopener noreferrer\">sshoker@openai.com<\/a> and Andrew Reddie at <a href=\"mailto:areddie@berkeley.edu\" rel=\"noopener noreferrer\">areddie@berkeley.edu<\/a>.<\/p><p>*Significant contribution, including writing, providing detailed input for the paper, research, workshop organization, and setting the direction of the paper.<br class=\"softbreak\">**Significant contribution, including providing detailed input for the paper, research, workshop organization, and setting the direction of the paper.<br class=\"softbreak\"><\/p>"}},"relationships":{"blocks":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/blocks","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/relationships\/blocks"}},"media":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/media","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/relationships\/media"},"data":[{"type":"media","id":"34529","meta":{"role":"cover","crop":"+1 (No bleed)","uuid":"169a9863-5725-45cf-b096-6d2e5b6cebe9\/confidence-building-measures-for-artificial-intelligence-workshop-proceedings.png"}},{"type":"media","id":"34530","meta":{"role":"cover","crop":"+2 (Full bleed)","uuid":"169a9863-5725-45cf-b096-6d2e5b6cebe9\/confidence-building-measures-for-artificial-intelligence-workshop-proceedings.png"}},{"type":"media","id":"34531","meta":{"role":"cover","crop":"listing","uuid":"169a9863-5725-45cf-b096-6d2e5b6cebe9\/confidence-building-measures-for-artificial-intelligence-workshop-proceedings.png"}}],"meta":{"roles":{"cover":{"+1 (No bleed)":["34529"],"+2 (Full bleed)":["34530"],"listing":["34531"]}}}},"files":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/files","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/relationships\/files"}},"related-items":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/related-items","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/relationships\/related-items"}},"ctas":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/ctas","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/relationships\/ctas"}},"research-publication-links":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/research-publication-links","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/relationships\/research-publication-links"}},"author-groups":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/author-groups","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/relationships\/author-groups"}},"topics":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/topics","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/relationships\/topics"}},"models":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/models","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/relationships\/models"}},"content-types":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/content-types","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/relationships\/content-types"}},"footnotes":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/footnotes","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/relationships\/footnotes"}},"references":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/references","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228\/relationships\/references"}}},"links":{"self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/228"}},{"type":"research-publications","id":"223","attributes":{"published":true,"createdAt":"2023-07-10T16:17:59.000000Z","updatedAt":"2023-07-10T17:21:55.000000Z","publicationDateFormatted":"July 6, 2023","publicationDate":"2023-07-06T00:00:00-07:00","slug":"frontier-ai-regulation","title":"Frontier AI regulation: Managing emerging risks to public safety","description":"","descriptionRichText":{"type":"div","props":[],"children":[{"type":"p","props":[],"children":[{"type":"br","props":{"class":"softbreak"},"children":[]}]}]},"heroBlendMode":"default","heroTitle":null,"template":"level-0","colorTheme":null,"releaseSummary":null,"releaseWhyItMatters":null,"releaseSafety":null,"chapters":false,"publicationAbstract":"<p>Advanced AI models hold the promise of tremendous benefits for humanity, but society needs to proactively manage the accompanying risks. In this paper, we focus on what we term \u201cfrontier AI\u201d models: highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety. Frontier AI models pose a distinct regulatory challenge: dangerous capabilities can arise unexpectedly; it is difficult to robustly prevent a deployed model from being misused; and, it is difficult to stop a model\u2019s capabilities from proliferating broadly. To address these challenges, at least three building blocks for the regulation of frontier models are needed: (1) standard-setting processes to identify appropriate requirements for frontier AI developers, (2) registration and reporting requirements to provide regulators with visibility into frontier AI development processes, and (3) mechanisms to ensure compliance with safety standards for the development and deployment of frontier AI models. Industry self-regulation is an important first step. However, wider societal discussions and government intervention will be needed to create standards and to ensure compliance with them. We consider several options to this end, including granting enforcement powers to supervisory authorities and licensure regimes for frontier AI models. Finally, we propose an initial set of safety standards. These include conducting pre-deployment risk assessments; external scrutiny of model behavior; using risk assessments to inform deployment decisions; and monitoring and responding to new information about model capabilities and uses post-deployment. We hope this discussion contributes to the broader conversation on how to balance public safety risks and innovation benefits from advances at the frontier of AI development.<br class=\"softbreak\"><\/p>","publicationAbstractRichText":{"type":"div","props":[],"children":[{"type":"p","props":[],"children":["Advanced AI models hold the promise of tremendous benefits for humanity, but society needs to proactively manage the accompanying risks. In this paper, we focus on what we term \u201cfrontier AI\u201d models: highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety. Frontier AI models pose a distinct regulatory challenge: dangerous capabilities can arise unexpectedly; it is difficult to robustly prevent a deployed model from being misused; and, it is difficult to stop a model\u2019s capabilities from proliferating broadly. To address these challenges, at least three building blocks for the regulation of frontier models are needed: (1) standard-setting processes to identify appropriate requirements for frontier AI developers, (2) registration and reporting requirements to provide regulators with visibility into frontier AI development processes, and (3) mechanisms to ensure compliance with safety standards for the development and deployment of frontier AI models. Industry self-regulation is an important first step. However, wider societal discussions and government intervention will be needed to create standards and to ensure compliance with them. We consider several options to this end, including granting enforcement powers to supervisory authorities and licensure regimes for frontier AI models. Finally, we propose an initial set of safety standards. These include conducting pre-deployment risk assessments; external scrutiny of model behavior; using risk assessments to inform deployment decisions; and monitoring and responding to new information about model capabilities and uses post-deployment. We hope this discussion contributes to the broader conversation on how to balance public safety risks and innovation benefits from advances at the frontier of AI development.",{"type":"br","props":{"class":"softbreak"},"children":[]}]}]},"seo":{"ogImageSrc":"https:\/\/images.openai.com\/blob\/d4f299e7-2084-43d6-80f5-40e9404f15e2\/frontier-ai-regulation-managing-emerging-risks-to-public-safety.png?trim=0%2C0%2C0%2C0","ogImageAlt":"Frontier AI Regulation: Managing Emerging Risks To Public Safety","twitterImageSrc":null,"twitterImageAlt":null,"noindex":false,"description":"","ogTitle":"Frontier AI regulation: Managing emerging risks to public safety","ogDescription":"","twitterTitle":null,"twitterDescription":null,"twitterCardType":"summary_large_image"},"acknowledgments":{"title1":"Report authors, alphabetical order","text1":"<p>Markus Anderljung (Centre for the Governance of AI; Center for a New American Security) *\u2020<br class=\"softbreak\">Joslyn Barnhart (Google DeepMind) **<br class=\"softbreak\">Jade Leung (OpenAI) *<br class=\"softbreak\">Anton Korinek (Brookings Institution; University of Virginia; Centre for the Governance of AI) **\u2020<br class=\"softbreak\">Cullen O\u2019Keefe (OpenAI) *<br class=\"softbreak\">Jess Whittlestone (Centre for the Study of Existential Risk, Univeristy of Cambridge) **<br class=\"softbreak\"><\/p>","title2":"Report authors, alphabetical order","text2":"<p>Shahar Avin (Centre for the Study of Existential Risk, Univeristy of Cambridge)<br class=\"softbreak\">Miles Brundage (OpenAI)<br class=\"softbreak\">Justin Bullock (University of Washington; Convergence Analysis)<br class=\"softbreak\">Duncan Cass-Beggs (Centre for International Governance Innovation)<br class=\"softbreak\">Ben Chang (The Andrew W. Marshall Foundation)<br class=\"softbreak\">Tantum Collins (GETTING-Plurality<br class=\"softbreak\">Network, Edmond &amp; Lily Safra Center for Ethics; Harvard University)<br class=\"softbreak\">Tim Fist (Center for a New American Security)<br class=\"softbreak\">Gillian Hadfield (University of Toronto; Vector Institute; OpenAI)<br class=\"softbreak\">Alan Hayes (Akin Gump Strauss Hauer &amp; Feld LLP)<br class=\"softbreak\">Lewis Ho (Google DeepMind)<br class=\"softbreak\">Sara Hooker (Cohere For AI)<br class=\"softbreak\">Eric Horvitz (Microsoft)<br class=\"softbreak\">Noam Kolt (University of Toronto)<br class=\"softbreak\">Jonas Schuett (Centre for the Governance of AI)<br class=\"softbreak\">Yonadav Shavit (Harvard University) ***<br class=\"softbreak\">Divya Siddarth (Collective Intelligence Project)<br class=\"softbreak\">Robert Trager (Centre for the Governance of AI; University of California: Los Angeles)<br class=\"softbreak\">Kevin Wolf (Akin Gump Strauss Hauer &amp; Feld LLP)<\/p><p><br><\/p><p>Listed authors contributed substantive ideas and\/or work to the white paper. Contributions include writing, editing, research, detailed feedback, and participation in a workshop on a draft of the paper. Given the size of the group, inclusion as an author does not entail endorsement of all claims in the paper, nor does inclusion entail an endorsement on the part of any individual\u2019s organization.<\/p><p>*Significant contribution, including writing, research, convening, and setting the direction of the paper.<br class=\"softbreak\">**Significant contribution, including editing, convening, detailed input, and setting the direction of the paper.<br class=\"softbreak\">***Work done while an independent contractor for OpenAI.<br class=\"softbreak\">\u2020Corresponding authors. Markus Anderljung (<a href=\"mailto:markus.anderljung@governance.ai\" rel=\"noopener noreferrer\">markus.anderljung@governance.ai<\/a>) and Anton Korinek (<a href=\"mailto:akorinek@brookings.edu\" rel=\"noopener noreferrer\">akorinek@brookings.edu<\/a>).<br class=\"softbreak\"><\/p>"}},"relationships":{"blocks":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/blocks","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/relationships\/blocks"}},"media":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/media","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/relationships\/media"},"data":[{"type":"media","id":"34080","meta":{"role":"cover","crop":"+1 (No bleed)","uuid":"d4f299e7-2084-43d6-80f5-40e9404f15e2\/frontier-ai-regulation-managing-emerging-risks-to-public-safety.png"}},{"type":"media","id":"34081","meta":{"role":"cover","crop":"+2 (Full bleed)","uuid":"d4f299e7-2084-43d6-80f5-40e9404f15e2\/frontier-ai-regulation-managing-emerging-risks-to-public-safety.png"}},{"type":"media","id":"34082","meta":{"role":"cover","crop":"listing","uuid":"d4f299e7-2084-43d6-80f5-40e9404f15e2\/frontier-ai-regulation-managing-emerging-risks-to-public-safety.png"}}],"meta":{"roles":{"cover":{"+1 (No bleed)":["34080"],"+2 (Full bleed)":["34081"],"listing":["34082"]}}}},"files":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/files","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/relationships\/files"}},"related-items":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/related-items","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/relationships\/related-items"}},"ctas":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/ctas","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/relationships\/ctas"}},"research-publication-links":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/research-publication-links","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/relationships\/research-publication-links"}},"author-groups":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/author-groups","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/relationships\/author-groups"}},"topics":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/topics","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/relationships\/topics"}},"models":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/models","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/relationships\/models"}},"content-types":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/content-types","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/relationships\/content-types"}},"footnotes":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/footnotes","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/relationships\/footnotes"}},"references":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/references","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223\/relationships\/references"}}},"links":{"self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/223"}},{"type":"research-publications","id":"220","attributes":{"published":true,"createdAt":"2023-05-30T16:52:02.000000Z","updatedAt":"2023-05-31T23:23:40.000000Z","publicationDateFormatted":"May 31, 2023","publicationDate":"2023-05-31T00:00:00-07:00","slug":"improving-mathematical-reasoning-with-process-supervision","title":"Improving mathematical reasoning with process supervision","description":"We've trained a model to achieve a new state-of-the-art in mathematical problem solving by rewarding each correct step of reasoning (\u201cprocess supervision\u201d) instead of simply rewarding the correct final answer (\u201coutcome supervision\u201d). In addition to boosting performance relative to outcome supervision, process supervision also has an important alignment benefit: it directly trains the model to produce a chain-of-thought that is endorsed by humans.","descriptionRichText":{"type":"div","props":[],"children":[{"type":"p","props":[],"children":["We've trained a model to achieve a new state-of-the-art in mathematical problem solving by rewarding each correct step of reasoning (\u201cprocess supervision\u201d) instead of simply rewarding the correct final answer (\u201coutcome supervision\u201d). In addition to boosting performance relative to outcome supervision, process supervision also has an important alignment benefit: it directly trains the model to produce a chain-of-thought that is endorsed by humans.",{"type":"br","props":{"class":"softbreak"},"children":[]}]}]},"heroBlendMode":"default","heroTitle":null,"template":"level-1","colorTheme":"mid-red","releaseSummary":null,"releaseWhyItMatters":null,"releaseSafety":null,"chapters":false,"publicationAbstract":null,"publicationAbstractRichText":null,"seo":{"ogImageSrc":"https:\/\/images.openai.com\/blob\/373bf52a-5373-4d4e-88fe-7fbf738ec6d1\/improving-mathematical-reasoning-with-process-supervision.jpg?trim=0%2C0%2C0%2C0","ogImageAlt":"Improving Mathematical Reasoning With Process Supervision","twitterImageSrc":null,"twitterImageAlt":null,"noindex":false,"description":"We've trained a model to achieve a new state-of-the-art in mathematical problem solving by rewarding each correct step of reasoning (\u201cprocess supervision\u201d) instead of simply rewarding the correct final answer (\u201coutcome supervision\u201d). In addition to boosting performance relative to outcome supervision, process supervision also has an important alignment benefit: it directly trains the model to produce a chain-of-thought that is endorsed by humans.","ogTitle":"Improving mathematical reasoning with process supervision","ogDescription":"We've trained a model to achieve a new state-of-the-art in mathematical problem solving by rewarding each correct step of reasoning (\u201cprocess supervision\u201d) instead of simply rewarding the correct final answer (\u201coutcome supervision\u201d). In addition to boosting performance relative to outcome supervision, process supervision also has an important alignment benefit: it directly trains the model to produce a chain-of-thought that is endorsed by humans.","twitterTitle":null,"twitterDescription":null,"twitterCardType":"summary_large_image"},"acknowledgments":{"title1":"Contributors","text1":"<p>Bowen Baker, Teddy Lee, John Schulman, Greg Brockman, Kendra Rimbach, Hannah Wong, Thomas Degry<br class=\"softbreak\"><\/p>","title2":null,"text2":null}},"relationships":{"blocks":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/blocks","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/relationships\/blocks"}},"media":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/media","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/relationships\/media"},"data":[{"type":"media","id":"32657","meta":{"role":"cover","crop":"+1 (No bleed)","uuid":"373bf52a-5373-4d4e-88fe-7fbf738ec6d1\/improving-mathematical-reasoning-with-process-supervision.jpg"}},{"type":"media","id":"32658","meta":{"role":"cover","crop":"+2 (Full bleed)","uuid":"373bf52a-5373-4d4e-88fe-7fbf738ec6d1\/improving-mathematical-reasoning-with-process-supervision.jpg"}},{"type":"media","id":"32659","meta":{"role":"cover","crop":"listing","uuid":"373bf52a-5373-4d4e-88fe-7fbf738ec6d1\/improving-mathematical-reasoning-with-process-supervision.jpg"}}],"meta":{"roles":{"cover":{"+1 (No bleed)":["32657"],"+2 (Full bleed)":["32658"],"listing":["32659"]}}}},"files":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/files","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/relationships\/files"}},"related-items":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/related-items","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/relationships\/related-items"}},"ctas":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/ctas","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/relationships\/ctas"}},"research-publication-links":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/research-publication-links","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/relationships\/research-publication-links"}},"author-groups":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/author-groups","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/relationships\/author-groups"}},"topics":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/topics","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/relationships\/topics"}},"models":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/models","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/relationships\/models"}},"content-types":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/content-types","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/relationships\/content-types"}},"footnotes":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/footnotes","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/relationships\/footnotes"}},"references":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/references","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220\/relationships\/references"}}},"links":{"self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/220"}},{"type":"research-publications","id":"219","attributes":{"published":true,"createdAt":"2023-04-27T22:10:59.000000Z","updatedAt":"2023-05-10T16:59:53.000000Z","publicationDateFormatted":"May 9, 2023","publicationDate":"2023-05-09T00:00:00-07:00","slug":"language-models-can-explain-neurons-in-language-models","title":"Language models can explain neurons in language models","description":"We use GPT-4 to automatically write explanations for the behavior of neurons in large language models and to score those explanations. We release a dataset of these (imperfect) explanations and scores for every neuron in GPT-2.","descriptionRichText":{"type":"div","props":[],"children":[{"type":"p","props":[],"children":["We use GPT-4 to automatically write explanations for the behavior of neurons in large language models and to score those explanations. We release a dataset of these (imperfect) explanations and scores for every neuron in GPT-2.",{"type":"br","props":{"class":"softbreak"},"children":[]}]}]},"heroBlendMode":"default","heroTitle":null,"template":"level-1","colorTheme":"mid-green","releaseSummary":null,"releaseWhyItMatters":null,"releaseSafety":null,"chapters":false,"publicationAbstract":null,"publicationAbstractRichText":null,"seo":{"ogImageSrc":"https:\/\/images.openai.com\/blob\/e1afc745-b554-4785-ad0e-8f9c65e1274f\/language-models-can-explain-neurons-in-language-models.png?trim=0%2C0%2C1791%2C0","ogImageAlt":"Language Models Can Explain Neurons In Language Models","twitterImageSrc":null,"twitterImageAlt":null,"noindex":false,"description":"We use GPT-4 to automatically write explanations for the behavior of neurons in large language models and to score those explanations. We release a dataset of these (imperfect) explanations and scores for every neuron in GPT-2.","ogTitle":"Language models can explain neurons in language models","ogDescription":"We use GPT-4 to automatically write explanations for the behavior of neurons in large language models and to score those explanations. We release a dataset of these (imperfect) explanations and scores for every neuron in GPT-2.","twitterTitle":null,"twitterDescription":null,"twitterCardType":"summary_large_image"},"acknowledgments":{"title1":"Figures","text1":"<p>Thomas Degry<br class=\"softbreak\">Nick Cammarata<br class=\"softbreak\"><\/p>","title2":"Suggestions","text2":"<p>Hannah Wong<br class=\"softbreak\">Greg Brockman<br class=\"softbreak\">Ilya Sutskever<br class=\"softbreak\">Kendra Rimbach<br class=\"softbreak\"><\/p>"}},"relationships":{"blocks":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/blocks","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/relationships\/blocks"}},"media":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/media","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/relationships\/media"},"data":[{"type":"media","id":"32284","meta":{"role":"cover","crop":"+1 (No bleed)","uuid":"e1afc745-b554-4785-ad0e-8f9c65e1274f\/language-models-can-explain-neurons-in-language-models.png"}},{"type":"media","id":"32285","meta":{"role":"cover","crop":"+2 (Full bleed)","uuid":"e1afc745-b554-4785-ad0e-8f9c65e1274f\/language-models-can-explain-neurons-in-language-models.png"}},{"type":"media","id":"32286","meta":{"role":"cover","crop":"listing","uuid":"e1afc745-b554-4785-ad0e-8f9c65e1274f\/language-models-can-explain-neurons-in-language-models.png"}}],"meta":{"roles":{"cover":{"+1 (No bleed)":["32284"],"+2 (Full bleed)":["32285"],"listing":["32286"]}}}},"files":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/files","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/relationships\/files"}},"related-items":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/related-items","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/relationships\/related-items"}},"ctas":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/ctas","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/relationships\/ctas"}},"research-publication-links":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/research-publication-links","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/relationships\/research-publication-links"}},"author-groups":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/author-groups","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/relationships\/author-groups"}},"topics":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/topics","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/relationships\/topics"}},"models":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/models","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/relationships\/models"}},"content-types":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/content-types","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/relationships\/content-types"}},"footnotes":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/footnotes","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/relationships\/footnotes"}},"references":{"links":{"related":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/references","self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219\/relationships\/references"}}},"links":{"self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/research-publications\/219"}}],"included":[{"type":"media","id":"34529","attributes":{"createdAt":"2023-08-03T18:44:59.000000Z","updatedAt":"2023-08-03T18:44:59.000000Z","uuid":"169a9863-5725-45cf-b096-6d2e5b6cebe9\/confidence-building-measures-for-artificial-intelligence-workshop-proceedings.png","filename":"confidence-building-measures-for-artificial-intelligence-workshop-proceedings.png","role":"cover","crop":"+1 (No bleed)","ratio":"1:1","lqip":null,"src":"https:\/\/images.openai.com\/blob\/169a9863-5725-45cf-b096-6d2e5b6cebe9\/confidence-building-measures-for-artificial-intelligence-workshop-proceedings.png?trim=0%2C0%2C0%2C0","originalSrc":"https:\/\/openaicomproductionae4b.blob.core.windows.net\/production-twill-01\/169a9863-5725-45cf-b096-6d2e5b6cebe9\/confidence-building-measures-for-artificial-intelligence-workshop-proceedings.png","width":2064,"height":2064,"alt":"Confidence Building Measures For Artificial Intelligence: Workshop Proceedings","caption":"","video":"","metadata":[]},"links":{"self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/media\/34529"},"meta":{"role":"cover","crop":"+1 (No bleed)","uuid":"169a9863-5725-45cf-b096-6d2e5b6cebe9\/confidence-building-measures-for-artificial-intelligence-workshop-proceedings.png"}},{"type":"media","id":"34530","attributes":{"createdAt":"2023-08-03T18:44:59.000000Z","updatedAt":"2023-08-03T18:44:59.000000Z","uuid":"169a9863-5725-45cf-b096-6d2e5b6cebe9\/confidence-building-measures-for-artificial-intelligence-workshop-proceedings.png","filename":"confidence-building-measures-for-artificial-intelligence-workshop-proceedings.png","role":"cover","crop":"+2 (Full bleed)","ratio":"default","lqip":null,"src":"https:\/\/images.openai.com\/blob\/169a9863-5725-45cf-b096-6d2e5b6cebe9\/confidence-building-measures-for-artificial-intelligence-workshop-proceedings.png?trim=0%2C0%2C0%2C0","originalSrc":"https:\/\/openaicomproductionae4b.blob.core.windows.net\/production-twill-01\/169a9863-5725-45cf-b096-6d2e5b6cebe9\/confidence-building-measures-for-artificial-intelligence-workshop-proceedings.png","width":2064,"height":2064,"alt":"Confidence Building Measures For Artificial Intelligence: Workshop Proceedings","caption":"","video":"","metadata":[]},"links":{"self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/media\/34530"},"meta":{"role":"cover","crop":"+2 (Full bleed)","uuid":"169a9863-5725-45cf-b096-6d2e5b6cebe9\/confidence-building-measures-for-artificial-intelligence-workshop-proceedings.png"}},{"type":"media","id":"34531","attributes":{"createdAt":"2023-08-03T18:44:59.000000Z","updatedAt":"2023-08-03T18:44:59.000000Z","uuid":"169a9863-5725-45cf-b096-6d2e5b6cebe9\/confidence-building-measures-for-artificial-intelligence-workshop-proceedings.png","filename":"confidence-building-measures-for-artificial-intelligence-workshop-proceedings.png","role":"cover","crop":"listing","ratio":"default","lqip":null,"src":"https:\/\/images.openai.com\/blob\/169a9863-5725-45cf-b096-6d2e5b6cebe9\/confidence-building-measures-for-artificial-intelligence-workshop-proceedings.png?trim=0%2C0%2C0%2C0","originalSrc":"https:\/\/openaicomproductionae4b.blob.core.windows.net\/production-twill-01\/169a9863-5725-45cf-b096-6d2e5b6cebe9\/confidence-building-measures-for-artificial-intelligence-workshop-proceedings.png","width":2064,"height":2064,"alt":"Confidence Building Measures For Artificial Intelligence: Workshop Proceedings","caption":"","video":"","metadata":[]},"links":{"self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/media\/34531"},"meta":{"role":"cover","crop":"listing","uuid":"169a9863-5725-45cf-b096-6d2e5b6cebe9\/confidence-building-measures-for-artificial-intelligence-workshop-proceedings.png"}},{"type":"media","id":"34080","attributes":{"createdAt":"2023-07-10T17:21:55.000000Z","updatedAt":"2023-07-10T17:21:55.000000Z","uuid":"d4f299e7-2084-43d6-80f5-40e9404f15e2\/frontier-ai-regulation-managing-emerging-risks-to-public-safety.png","filename":"frontier-ai-regulation-managing-emerging-risks-to-public-safety.png","role":"cover","crop":"+1 (No bleed)","ratio":"1:1","lqip":null,"src":"https:\/\/images.openai.com\/blob\/d4f299e7-2084-43d6-80f5-40e9404f15e2\/frontier-ai-regulation-managing-emerging-risks-to-public-safety.png?trim=0%2C0%2C0%2C0","originalSrc":"https:\/\/openaicomproductionae4b.blob.core.windows.net\/production-twill-01\/d4f299e7-2084-43d6-80f5-40e9404f15e2\/frontier-ai-regulation-managing-emerging-risks-to-public-safety.png","width":2064,"height":2064,"alt":"Frontier AI Regulation: Managing Emerging Risks To Public Safety","caption":"","video":"","metadata":[]},"links":{"self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/media\/34080"},"meta":{"role":"cover","crop":"+1 (No bleed)","uuid":"d4f299e7-2084-43d6-80f5-40e9404f15e2\/frontier-ai-regulation-managing-emerging-risks-to-public-safety.png"}},{"type":"media","id":"34081","attributes":{"createdAt":"2023-07-10T17:21:55.000000Z","updatedAt":"2023-07-10T17:21:55.000000Z","uuid":"d4f299e7-2084-43d6-80f5-40e9404f15e2\/frontier-ai-regulation-managing-emerging-risks-to-public-safety.png","filename":"frontier-ai-regulation-managing-emerging-risks-to-public-safety.png","role":"cover","crop":"+2 (Full bleed)","ratio":"default","lqip":null,"src":"https:\/\/images.openai.com\/blob\/d4f299e7-2084-43d6-80f5-40e9404f15e2\/frontier-ai-regulation-managing-emerging-risks-to-public-safety.png?trim=0%2C0%2C0%2C0","originalSrc":"https:\/\/openaicomproductionae4b.blob.core.windows.net\/production-twill-01\/d4f299e7-2084-43d6-80f5-40e9404f15e2\/frontier-ai-regulation-managing-emerging-risks-to-public-safety.png","width":2064,"height":2064,"alt":"Frontier AI Regulation: Managing Emerging Risks To Public Safety","caption":"","video":"","metadata":[]},"links":{"self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/media\/34081"},"meta":{"role":"cover","crop":"+2 (Full bleed)","uuid":"d4f299e7-2084-43d6-80f5-40e9404f15e2\/frontier-ai-regulation-managing-emerging-risks-to-public-safety.png"}},{"type":"media","id":"34082","attributes":{"createdAt":"2023-07-10T17:21:55.000000Z","updatedAt":"2023-07-10T17:21:55.000000Z","uuid":"d4f299e7-2084-43d6-80f5-40e9404f15e2\/frontier-ai-regulation-managing-emerging-risks-to-public-safety.png","filename":"frontier-ai-regulation-managing-emerging-risks-to-public-safety.png","role":"cover","crop":"listing","ratio":"default","lqip":null,"src":"https:\/\/images.openai.com\/blob\/d4f299e7-2084-43d6-80f5-40e9404f15e2\/frontier-ai-regulation-managing-emerging-risks-to-public-safety.png?trim=0%2C0%2C0%2C0","originalSrc":"https:\/\/openaicomproductionae4b.blob.core.windows.net\/production-twill-01\/d4f299e7-2084-43d6-80f5-40e9404f15e2\/frontier-ai-regulation-managing-emerging-risks-to-public-safety.png","width":2064,"height":2064,"alt":"Frontier AI Regulation: Managing Emerging Risks To Public Safety","caption":"","video":"","metadata":[]},"links":{"self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/media\/34082"},"meta":{"role":"cover","crop":"listing","uuid":"d4f299e7-2084-43d6-80f5-40e9404f15e2\/frontier-ai-regulation-managing-emerging-risks-to-public-safety.png"}},{"type":"media","id":"32657","attributes":{"createdAt":"2023-06-01T16:17:38.000000Z","updatedAt":"2023-06-01T16:17:38.000000Z","uuid":"373bf52a-5373-4d4e-88fe-7fbf738ec6d1\/improving-mathematical-reasoning-with-process-supervision.jpg","filename":"improving-mathematical-reasoning-with-process-supervision.jpg","role":"cover","crop":"+1 (No bleed)","ratio":"16:9","lqip":null,"src":"https:\/\/images.openai.com\/blob\/373bf52a-5373-4d4e-88fe-7fbf738ec6d1\/improving-mathematical-reasoning-with-process-supervision.jpg?trim=0%2C0%2C0%2C0","originalSrc":"https:\/\/openaicomproductionae4b.blob.core.windows.net\/production-twill-01\/373bf52a-5373-4d4e-88fe-7fbf738ec6d1\/improving-mathematical-reasoning-with-process-supervision.jpg","width":8192,"height":4608,"alt":"Improving Mathematical Reasoning With Process Supervision","caption":"<p>Illustration: Ruby Chen<br class=\"softbreak\"><\/p>","video":"","metadata":[]},"links":{"self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/media\/32657"},"meta":{"role":"cover","crop":"+1 (No bleed)","uuid":"373bf52a-5373-4d4e-88fe-7fbf738ec6d1\/improving-mathematical-reasoning-with-process-supervision.jpg"}},{"type":"media","id":"32658","attributes":{"createdAt":"2023-06-01T16:17:38.000000Z","updatedAt":"2023-06-01T16:17:38.000000Z","uuid":"373bf52a-5373-4d4e-88fe-7fbf738ec6d1\/improving-mathematical-reasoning-with-process-supervision.jpg","filename":"improving-mathematical-reasoning-with-process-supervision.jpg","role":"cover","crop":"+2 (Full bleed)","ratio":"default","lqip":null,"src":"https:\/\/images.openai.com\/blob\/373bf52a-5373-4d4e-88fe-7fbf738ec6d1\/improving-mathematical-reasoning-with-process-supervision.jpg?trim=0%2C3439%2C0%2C144","originalSrc":"https:\/\/openaicomproductionae4b.blob.core.windows.net\/production-twill-01\/373bf52a-5373-4d4e-88fe-7fbf738ec6d1\/improving-mathematical-reasoning-with-process-supervision.jpg","width":4609,"height":4608,"alt":"Improving Mathematical Reasoning With Process Supervision","caption":"<p>Illustration: Ruby Chen<br class=\"softbreak\"><\/p>","video":"","metadata":[]},"links":{"self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/media\/32658"},"meta":{"role":"cover","crop":"+2 (Full bleed)","uuid":"373bf52a-5373-4d4e-88fe-7fbf738ec6d1\/improving-mathematical-reasoning-with-process-supervision.jpg"}},{"type":"media","id":"32659","attributes":{"createdAt":"2023-06-01T16:17:38.000000Z","updatedAt":"2023-06-01T16:17:38.000000Z","uuid":"373bf52a-5373-4d4e-88fe-7fbf738ec6d1\/improving-mathematical-reasoning-with-process-supervision.jpg","filename":"improving-mathematical-reasoning-with-process-supervision.jpg","role":"cover","crop":"listing","ratio":"default","lqip":null,"src":"https:\/\/images.openai.com\/blob\/373bf52a-5373-4d4e-88fe-7fbf738ec6d1\/improving-mathematical-reasoning-with-process-supervision.jpg?trim=0%2C3439%2C0%2C144","originalSrc":"https:\/\/openaicomproductionae4b.blob.core.windows.net\/production-twill-01\/373bf52a-5373-4d4e-88fe-7fbf738ec6d1\/improving-mathematical-reasoning-with-process-supervision.jpg","width":4609,"height":4608,"alt":"Improving Mathematical Reasoning With Process Supervision","caption":"<p>Illustration: Ruby Chen<br class=\"softbreak\"><\/p>","video":"","metadata":[]},"links":{"self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/media\/32659"},"meta":{"role":"cover","crop":"listing","uuid":"373bf52a-5373-4d4e-88fe-7fbf738ec6d1\/improving-mathematical-reasoning-with-process-supervision.jpg"}},{"type":"media","id":"32284","attributes":{"createdAt":"2023-05-10T16:59:54.000000Z","updatedAt":"2023-05-10T16:59:54.000000Z","uuid":"e1afc745-b554-4785-ad0e-8f9c65e1274f\/language-models-can-explain-neurons-in-language-models.png","filename":"language-models-can-explain-neurons-in-language-models.png","role":"cover","crop":"+1 (No bleed)","ratio":"16:9","lqip":null,"src":"https:\/\/images.openai.com\/blob\/e1afc745-b554-4785-ad0e-8f9c65e1274f\/language-models-can-explain-neurons-in-language-models.png?trim=0%2C0%2C1791%2C0","originalSrc":"https:\/\/openaicomproductionae4b.blob.core.windows.net\/production-twill-01\/e1afc745-b554-4785-ad0e-8f9c65e1274f\/language-models-can-explain-neurons-in-language-models.png","width":4096,"height":2305,"alt":"Language Models Can Explain Neurons In Language Models","caption":"<p>Illustration: Ruby Chen<br class=\"softbreak\"><\/p>","video":"","metadata":[]},"links":{"self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/media\/32284"},"meta":{"role":"cover","crop":"+1 (No bleed)","uuid":"e1afc745-b554-4785-ad0e-8f9c65e1274f\/language-models-can-explain-neurons-in-language-models.png"}},{"type":"media","id":"32285","attributes":{"createdAt":"2023-05-10T16:59:54.000000Z","updatedAt":"2023-05-10T16:59:54.000000Z","uuid":"e1afc745-b554-4785-ad0e-8f9c65e1274f\/language-models-can-explain-neurons-in-language-models.png","filename":"language-models-can-explain-neurons-in-language-models.png","role":"cover","crop":"+2 (Full bleed)","ratio":"default","lqip":null,"src":"https:\/\/images.openai.com\/blob\/e1afc745-b554-4785-ad0e-8f9c65e1274f\/language-models-can-explain-neurons-in-language-models.png?trim=0%2C0%2C0%2C0","originalSrc":"https:\/\/openaicomproductionae4b.blob.core.windows.net\/production-twill-01\/e1afc745-b554-4785-ad0e-8f9c65e1274f\/language-models-can-explain-neurons-in-language-models.png","width":4096,"height":4096,"alt":"Language Models Can Explain Neurons In Language Models","caption":"<p>Illustration: Ruby Chen<br class=\"softbreak\"><\/p>","video":"","metadata":[]},"links":{"self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/media\/32285"},"meta":{"role":"cover","crop":"+2 (Full bleed)","uuid":"e1afc745-b554-4785-ad0e-8f9c65e1274f\/language-models-can-explain-neurons-in-language-models.png"}},{"type":"media","id":"32286","attributes":{"createdAt":"2023-05-10T16:59:54.000000Z","updatedAt":"2023-05-10T16:59:54.000000Z","uuid":"e1afc745-b554-4785-ad0e-8f9c65e1274f\/language-models-can-explain-neurons-in-language-models.png","filename":"language-models-can-explain-neurons-in-language-models.png","role":"cover","crop":"listing","ratio":"default","lqip":null,"src":"https:\/\/images.openai.com\/blob\/e1afc745-b554-4785-ad0e-8f9c65e1274f\/language-models-can-explain-neurons-in-language-models.png?trim=0%2C0%2C0%2C0","originalSrc":"https:\/\/openaicomproductionae4b.blob.core.windows.net\/production-twill-01\/e1afc745-b554-4785-ad0e-8f9c65e1274f\/language-models-can-explain-neurons-in-language-models.png","width":4096,"height":4096,"alt":"Language Models Can Explain Neurons In Language Models","caption":"<p>Illustration: Ruby Chen<br class=\"softbreak\"><\/p>","video":"","metadata":[]},"links":{"self":"https:\/\/openaicom-api-bdcpf8c6d2e9atf6.z01.azurefd.net\/api\/v1\/media\/32286"},"meta":{"role":"cover","crop":"listing","uuid":"e1afc745-b554-4785-ad0e-8f9c65e1274f\/language-models-can-explain-neurons-in-language-models.png"}}]}